{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Math primer for ML & TensorFlow workshop\n",
    "\n",
    "This notebook is meant to provide some basic mathematical backgrounds for the Python ML & TensorFlow Workshop.  Three topics are covered:   \n",
    "\n",
    "1. Structure in Data\n",
    "2. Basic Concepts in TensorFlow\n",
    "3. Loss Function and Gradient Descent\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Structure in Data\n",
    "\n",
    "Intuitively speaking, structure in data means two properties:\n",
    "* Given some data, one can predict the other data points with some confidence \n",
    "* One can compress the data, i.e., store the same amount of information, with less space\n",
    "\n",
    "For example, consider two arrays:\n",
    "\n",
    "A = (1, 2, 6, 2, 4, 7)\n",
    "\n",
    "B = (1, 2, 1, 2, 1, 2)\n",
    "\n",
    "We might say that A does not have apparent structure, whereas B does.\n",
    "\n",
    "### Entropy\n",
    "\n",
    "There are several ways of characterizing \"structure\" in data.  For example, if elements in A and B are draw from probability distributions, then we can define entropy as:\n",
    "\n",
    "$$ H(X)\\;=\\;- \\sum^N_{i=1} p(x_i) \\; log_2 p(x_i) $$\n",
    "\n",
    "To interpret this definition, consider two simple processes: a coin toss and a dice roll.  In each scenario, the outcomes are equally likely, but the numbers of possible outcomes are 2 and 6.  Therefore (in the unit of bits):\n",
    "\n",
    "$$ H(coin\\;toss)\\;=\\;- \\sum^2_{i=1} {1 \\over 2} \\; log_2 {1 \\over 2} \\;=\\; 1.0 $$\n",
    "$$ H(dice\\;roll)\\;=\\;- \\sum^6_{i=1} {1 \\over 6} \\; log_2 {1 \\over 6} \\;=\\; 2.58 $$\n",
    "\n",
    "So the dice roll process has a higher entropy than coin toss; predicting dice roll outcomes is more difficult, i.e., incurs a larger uncertainty.  Now compare a fair coin toss and a biased coin toss (that, say, returns heads 80% of the time):\n",
    "\n",
    "$$ H(50/50\\;coin\\;toss)\\;=\\;- \\sum^2_{i=1} {1 \\over 2} \\; log_2 {1 \\over 2} \\;=\\; 1.0 $$\n",
    "$$ H(20/80\\;coin\\;toss)\\;=\\;- {1 \\over 5} \\; log_2 {1 \\over 5} - {4 \\over 5} \\; log_2 {4 \\over 5} \\;=\\; 0.72 $$\n",
    "\n",
    "So a biased coin toss has lower entropy; predicting its outcome is easier (i.e., less uncertainty) than a fair coin toss.   \n",
    "\n",
    "#### Exercise\n",
    "To continue with the arrays A and B example above, assume these two arrays are representative of the distributions that generated the elements.  Compute the entropy for the distributions that generated A and B. (Note: in real world one almost never knows the actual distribution of a data-generating process.)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic concepts for TensorFlow\n",
    "\n",
    "You might recall from linear algebra that arrays have the following names depending on their dimensionality:\n",
    "\n",
    "* Scalar - an array in 0-D\n",
    "* Vector - an array in 1-D\n",
    "* Matrix - an array in 2-D\n",
    "\n",
    "They are all **tensors** of n-th order.  That is, a tensor is an array with arbitrary dimensionality.  Just like how scalars, vectors, and matrices can undergo mathematical operations such as addition and multiplication, tensors can be transformed with operations as well, provided that the input tensors have compatible shapes (for example, you can only add vectors with same number of elements, and multiply an NxM matrix with one that's MxP).  \n",
    "\n",
    "TensorFlow provides a library of algorithms to perform tensor operations efficiently, which are fundemental calculation tasks in machine learning.  For example, consider a simple linear regression model with a single feature:     \n",
    "\n",
    "$$ w_0 + w_1 x \\;=\\; \\hat{y} $$\n",
    "\n",
    "where standard notations are used: $x$ is the feature, $w_0$ and $w_1$ are the **weights** (or parameters) determined during model training, and $\\hat{y}$ is the predicted outcome, to be compared with actual observations $y$.  The goal of building a model is to find values of $w_0$ and $w_1$ that minimize prediction error (but also balanced by the model being generalizable, i.e., not over-fit).  \n",
    "\n",
    "### Graph representation of ML models\n",
    "\n",
    "We can represent linear regression as a **graph**, where data flow from one node to another to undergo mathematical operations.  In this representation, we have\n",
    "\n",
    "<img src=\"./linear_reg_graph.png\" width=\"20%\" height=\"20%\">\n",
    "\n",
    "\n",
    "Consider a slightly larger neural net graph:\n",
    "\n",
    "<img src=\"./nnet_graph.png\" width=\"30%\" height=\"30%\">\n",
    "\n",
    "In this graph, there are three features $x_1$, $x_2$, and $x_3$, which are fed into **activation functions** $a_1^{(2)}$,  $a_2^{(2)}$, and $a_3^{(2)}$ in the second layer (the superscript denotes layer number and the subscript denotes the node number in that layer; note the number of nodes in each layer does not have to be the same as the number of features).  Not shown here are the weights $w$'s similar to the case of linear regression example above.  We can have different weights for different features, as well as for different activation functions.  Therefore, for each activation function $a_i^{(2)}$ in the second layer:\n",
    "\n",
    "$$ a_i^{(2)} \\;\\; = g( w_{i1} x_1 + w_{i2} x_2 + w_{i3} x_3 ),$$\n",
    "\n",
    "where $g(...)$ simply states that the activation function $a_i^{(2)}$ takes the input $w_{i1} x_1 + w_{i2} x_2 + w_{i3} x_3$.  In the simple linear regression example, we have the intercept term $w_0$; we can similarly implement it here in a neural net by adding $w_{i0}$, which is called a **biasing term**:\n",
    "\n",
    "$$ a_i^{(2)} \\; =\\;g( w_{i0} + w_{i1} x_1 + w_{i2} x_2 + w_{i3} x_3 ).$$\n",
    "\n",
    "After the operations at the second layer, the data is subsequently sent to a third layer, with activation functions $a_i^{(3)}$.  One can add nodes and layers to the neural net model; a **deep neural net** is one with many layers, perhaps 10's or 100's.  As you build more complex models using TensorFlow, it could be helpful to visualize your graph.  **[TensorBoard](https://www.tensorflow.org/versions/r0.7/how_tos/graph_viz/index.html)** provides this visualization tool.\n",
    "\n",
    "### Activation functions\n",
    "\n",
    "The activation functions can be different operations.  For example, if $g(...)$ is linear, then we return to linear regression.  So in practice $g(...)$ is typically non-linear, and a popular activation function is the rectified linear unit (**ReLU**):\n",
    "\n",
    "$$g(u)\\;=\\;max(0, u).$$\n",
    "\n",
    "\n",
    "\n",
    "### Model output\n",
    "\n",
    "After the data are pass through layers of operations, the output is generated.  The range of the output value depends on what activation function is used, but generally could be any real number $[-\\infty, +\\infty]$.  For categorical prediction, such as a binary classification of either 0 or 1, an additional sigmoid function can be applied to bring the final output within the range of $[0, 1]$, much like the case of logistic regression:\n",
    "\n",
    "$$ \\sigma(u) \\;=\\; { 1 \\over 1 + e^{-u} }.$$\n",
    "\n",
    "The empty node in the neural net graph above is meant to represent this last operation.  Often we need to make a multi-class prediction, for example, given an image, if the subject is cat, dog, human, car, etc.  In this case we need to transform the output from $[-\\infty, +\\infty]$ to one of the several categories, and a **softmax function** would be used:\n",
    "\n",
    "$$ S_j(\\textbf{u}) \\;=\\; { e^{u_j} \\over \\sum_{k=1}^K e^{u_k} },\\;\\;\\;\\;\\;j\\;\\in\\;[1, 2,... K].$$\n",
    "\n",
    "Here $\\textbf{u}$ is an array with $K$ elements ($u_1$, $u_2$, ... $u_K$), and the function $S$ transforms $\\textbf{u} \\in \\mathbb{R}^K$ to $[0, 1]^K$, while also observing the normalization condition\n",
    "\n",
    "$$ \\sum_{j=1}^K S_j(\\textbf{u})\\;=\\;1.$$\n",
    "\n",
    "In the case of multi-class prediction, each possible state is represented typically with **one-hot encoding** (also known as **dummy encoding** or **one-of-K**).  For example, with coin flip, $K$ = 2 and the two states would be (1,0) and (0,1); for dice roll, $K$ = 6 and the six states would be (1,0,0,0,0,0), (0,1,0,0,0,0), (0,0,1,0,0,0), etc.\n",
    "\n",
    "\n",
    "#### Exercise\n",
    "Show that the softmax function for $K$ = 1 is equivalent to a sigmoid function.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loss function and gradient descent\n",
    "\n",
    "In this case, the operations will be performed on $w_0$ and $w_1$ so that we can minimize prediction error, for example, an L2 loss of \n",
    "\n",
    "$$ L\\;=\\;\\sum (y - \\hat{y}) ^ 2 $$\n",
    "\n",
    "which is also known as squared errors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
